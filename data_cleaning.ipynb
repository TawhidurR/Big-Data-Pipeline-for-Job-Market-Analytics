{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2597c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Phase2_Test\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0263bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_raw rows: 1348488\n",
      "skills_raw rows: 1296381\n",
      "summ_raw rows: 48219735\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F;\n",
    "\n",
    "jobs_path = \"linkedin_job_postings.csv\";\n",
    "skills_path = \"job_skills.csv\";\n",
    "summ_path = \"job_summary.csv\";\n",
    "\n",
    "jobs_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(jobs_path)\n",
    ");\n",
    "\n",
    "skills_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(skills_path)\n",
    ");\n",
    "\n",
    "summ_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(summ_path)\n",
    ");\n",
    "\n",
    "def trim_cols(df):\n",
    "    return df.toDF(*[c.strip() for c in df.columns]);\n",
    "\n",
    "jobs = trim_cols(jobs_raw);\n",
    "skills = trim_cols(skills_raw);\n",
    "summ = trim_cols(summ_raw);\n",
    "\n",
    "print(\"jobs_raw rows:\", jobs.count());\n",
    "print(\"skills_raw rows:\", skills.count());\n",
    "print(\"summ_raw rows:\", summ.count());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d480496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows (before dedup): 1348488\n",
      "pct_missing_job_link: 0.0\n",
      "pct_missing_job_skills: 4.0129\n",
      "Rows before cleaning: 1348488\n",
      "Rows after cleaning: 1348463\n",
      "Duplicates found and removed: 25\n"
     ]
    }
   ],
   "source": [
    "jobs_sk = jobs.join(skills, on=\"job_link\", how=\"left\");\n",
    "\n",
    "total_rows = jobs_sk.count();\n",
    "missing_job_skills_count = jobs_sk.filter(F.col(\"job_skills\").isNull()).count();\n",
    "missing_job_link_count = jobs_sk.filter(F.col(\"job_link\").isNull()).count();\n",
    "\n",
    "pct_missing_job_link = (missing_job_link_count / total_rows) * 100;\n",
    "pct_missing_job_skills = (missing_job_skills_count / total_rows) * 100;\n",
    "\n",
    "rows_before = total_rows;\n",
    "\n",
    "dup_count = (\n",
    "    jobs_sk.select(\"job_link\").count()\n",
    "    - jobs_sk.select(\"job_link\").dropDuplicates([\"job_link\"]).count()\n",
    ");\n",
    "\n",
    "jobs_sk = jobs_sk.dropDuplicates([\"job_link\"]);\n",
    "\n",
    "rows_after = jobs_sk.count();\n",
    "\n",
    "print(\"Total rows (before dedup):\", rows_before);\n",
    "print(\"pct_missing_job_link:\", round(pct_missing_job_link, 4));\n",
    "print(\"pct_missing_job_skills:\", round(pct_missing_job_skills, 4));\n",
    "print(\"Rows before cleaning:\", rows_before);\n",
    "print(\"Rows after cleaning:\", rows_after);\n",
    "print(\"Duplicates found and removed:\", dup_count);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3e96a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type conversions and text cleanup.\n"
     ]
    }
   ],
   "source": [
    "truthy = [\"true\", \"t\", \"yes\", \"y\", \"1\"];\n",
    "falsy = [\"false\", \"f\", \"no\", \"n\", \"0\"];\n",
    "\n",
    "bool_cols = [\"got_summary\", \"got_ner\", \"is_being_worked\"];\n",
    "\n",
    "for col in bool_cols:\n",
    "    s = F.lower(F.trim(F.col(col).cast(\"string\")));\n",
    "    jobs_sk = jobs_sk.withColumn(\n",
    "        col,\n",
    "        F.when(s.isin(truthy), F.lit(1))\n",
    "         .when(s.isin(falsy), F.lit(0))\n",
    "         .otherwise(F.lit(0))\n",
    "    );\n",
    "\n",
    "text_cols = [\"job_link\", \"job_title\", \"company\",\n",
    "             \"job_location\", \"job_level\", \"job_type\"];\n",
    "\n",
    "for col in text_cols:\n",
    "    jobs_sk = jobs_sk.withColumn(\n",
    "        col,\n",
    "        F.when(F.col(col).isNull(), None).otherwise(\n",
    "            F.regexp_replace(F.col(col).cast(\"string\"), r\"\\s+\", \" \")\n",
    "        )\n",
    "    );\n",
    "    jobs_sk = jobs_sk.withColumn(col, F.trim(F.col(col)));\n",
    "\n",
    "print(\"type conversions and text cleanup.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d209aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_skills cleaning and text normalization.\n"
     ]
    }
   ],
   "source": [
    "jobs_sk = jobs_sk.withColumn(\n",
    "    \"job_skills_clean\",\n",
    "    F.when(F.col(\"job_skills\").isNull(), \"\")\n",
    "     .otherwise(F.lower(F.col(\"job_skills\").cast(\"string\")))\n",
    ");\n",
    "\n",
    "jobs_sk = jobs_sk.withColumn(\n",
    "    \"job_skills_clean\",\n",
    "    F.regexp_replace(\"job_skills_clean\", \"[;|]\", \",\")\n",
    ");\n",
    "\n",
    "jobs_sk = jobs_sk.withColumn(\n",
    "    \"job_skills_clean\",\n",
    "    F.regexp_replace(\"job_skills_clean\", r\"\\s*,\\s*\", \",\")\n",
    ");\n",
    "\n",
    "jobs_sk = jobs_sk.withColumn(\n",
    "    \"job_skills_clean\",\n",
    "    F.regexp_replace(\"job_skills_clean\", r\"\\s+\", \" \")\n",
    ");\n",
    "\n",
    "jobs_sk = jobs_sk.withColumn(\"skills_arr_raw\", F.split(\"job_skills_clean\", \",\"));\n",
    "\n",
    "jobs_sk = jobs_sk.withColumn(\n",
    "    \"skills_list\",\n",
    "    F.expr(\"filter(transform(skills_arr_raw, x -> trim(x)), x -> x <> '')\")\n",
    ");\n",
    "\n",
    "jobs_sk = jobs_sk.drop(\"skills_arr_raw\");\n",
    "\n",
    "jobs_sk = jobs_sk.withColumn(\"skill_count\", F.size(\"skills_list\"));\n",
    "\n",
    "clean_cols = [\n",
    "    \"job_link\", \"job_title\", \"company\",\n",
    "    \"job_location\", \"job_level\", \"job_type\",\n",
    "    \"job_skills_clean\"\n",
    "];\n",
    "\n",
    "for col in clean_cols:\n",
    "    jobs_sk = jobs_sk.withColumn(\n",
    "        col,\n",
    "        F.regexp_replace(F.col(col), r\"\\s+\", \" \")\n",
    "    );\n",
    "    jobs_sk = jobs_sk.withColumn(col, F.trim(F.col(col)));\n",
    "\n",
    "print(\"job_skills cleaning and text normalization.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5576a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows after cleaning: 1348463\n",
      "Total number of columns after cleaning: 18\n",
      "Percentage of missing values in job_link column: 0.0\n",
      "Percentage of missing values in job_skills column: 4.0129\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F;\n",
    "\n",
    "final_rows = rows_after;\n",
    "final_cols = len(jobs_sk.columns);\n",
    "\n",
    "print(\"Total number of rows after cleaning:\", final_rows);\n",
    "print(\"Total number of columns after cleaning:\", final_cols);\n",
    "print(\"Percentage of missing values in job_link column:\", round(pct_missing_job_link, 4));\n",
    "print(\"Percentage of missing values in job_skills column:\", round(pct_missing_job_skills, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad451505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F;\n",
    "\n",
    "rows_after = jobs_sk.count();\n",
    "num_columns_after = len(jobs_sk.columns);\n",
    "\n",
    "jobs_sk_export = jobs_sk.withColumn(\n",
    "    \"skills_list\",\n",
    "    F.concat_ws(\";\", F.col(\"skills_list\"))\n",
    ");\n",
    "\n",
    "output_dir = \"linkedin_jobs_skills_clean_pyspark\";\n",
    "\n",
    "(\n",
    "    jobs_sk_export\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(output_dir)\n",
    ");\n",
    "\n",
    "print(\"rows_after:\", rows_after);\n",
    "print(\"num_columns_after:\", num_columns_after);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
